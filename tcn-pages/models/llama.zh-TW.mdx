## LLaMA: 開源且高效的基礎語言模型

<Callout emoji="⚠️">
  本單元還在趕工中。
</Callout>


import {Screenshot} from 'components/screenshot'
import { Callout, FileTree } from 'nextra-theme-docs'
import LLAMA1 from '../../img/llama-1.png'


## 有什麼新鮮事？

這篇論文介紹一系列基礎語言模型，參數數量介於 7B ~ 65B 之間。

這些模型由公開的資料集訓練而成，總計達數兆個標記資料。

由 [Hoffman 等人 (2022)](https://arxiv.org/abs/2203.15556)的研究發現，在給定的運算資源預算下，以更小的模型在較多的資料上訓練，可以比使用更大的模型得到更好的表現。該研究也推薦，以 200B 個標記資料訓練擁有 10B 個參數的模型。然而 LLaMA 的論文發現，一個擁有 7B 個參數的模型，就算已經接受超過 1T 個標記資料的訓練，表現仍然可以持續提升。

<Screenshot src={LLAMA1} alt="LLAMA1" />

這個研究的重點，在於訓練模型 (LLaMA) 能在各類型推理任務下達成最好的表現，做法是透過使用更多標記資料進行訓練。


## 能力和關鍵結果 (CKR)

總結來說，LLaMA-13B 在許多基準測試上的表現超越 GPT-3 (175B)，而且還包含十分之一的模型大小和可以運行在單一 GPU 上的強大優勢。LLaMA-65B 則能與 Chinchilla-70B 和 PaLM-540B 這類的模型相抗衡。


*論文：* [LLaMA: Open and Efficient Foundation Language Models](https://arxiv.org/abs/2302.13971)

*程式碼：* 


## 參考

- [Koala: A Dialogue Model for Academic Research](https://bair.berkeley.edu/blog/2023/04/03/koala/) (April 2023)
- [Baize: An Open-Source Chat Model with Parameter-Efficient Tuning on Self-Chat Data](https://arxiv.org/abs/2304.01196) (April 2023)
- [Vicuna: An Open-Source Chatbot Impressing GPT-4 with 90%* ChatGPT Quality](https://vicuna.lmsys.org/) (March 2023)
- [LLaMA-Adapter: Efficient Fine-tuning of Language Models with Zero-init Attention](https://arxiv.org/abs/2303.16199) (March 2023)
- [GPT4All](https://github.com/nomic-ai/gpt4all) (March 2023)
- [ChatDoctor: A Medical Chat Model Fine-tuned on LLaMA Model using Medical Domain Knowledge](https://arxiv.org/abs/2303.14070) (March 2023)
- [Stanford Alpaca](https://github.com/tatsu-lab/stanford_alpaca) (March 2023)
