# 大型語言模型 (LLM) 的設定

在使用提示時，你會直接或是透過 API 和 LLM 互動。能藉由設定一些參數來讓提示得到不同的結果。

**Temperature** - 簡單說，越低的 `temperature` 得到的結果越確定，因為被選擇的下一個輸出總是機率最高那個。提高 `temperature` 會帶來更多隨機性，也就有更多樣化或創造性的輸出。你可能會需要提高其他參數的權重。實際應用中，當處理基於事實的問答任務時，你可能希望設定一個比較低的 `temperature` 參數值，以鼓勵模型給出更真實更簡潔的回應。而處理詩詞生成或其他創造性任務時，提高 `temperature` 的值則是一個比較有利的做法。

**Top_p** - `top_p` 是一個和 `temperature` 類似的採樣技術。也是一種被稱為`核心採樣 (nucleus sampling)`的溫度採樣技術，可以控制模型在生成回應階段的決定。如果你希望得到特定且真實的答案，那請將該參數調低；如果你希望得到更多樣化的回答，則提高參數值。

一般的建議是只調整其中一個，而非同時調整兩個。

在開始一些基本範例之前，請記住：你得出的結果很大程度上取決於使用得 LLM 版本。
