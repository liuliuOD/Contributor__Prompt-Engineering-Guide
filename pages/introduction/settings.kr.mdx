# LLM 설정

프롬프트를 사용 시 API를 사용하거나 직접 LLM과 소통할 것입니다. 몇 가지 파라미터를 설정함으로써 프롬프트에 별 다른 결과를 얻을 수 있습니다.

**Temperature** - 간단히 말해서, `temperature`값이 낮을수록 가장 확률이 높은 응답(token)이 항상 선택되기 때문에 결과는 더 결정론적입니다. 이 값을 올리면 랜덤성이 증가하여 보다 다양하고 창조적인 결과물을 배출합니다. 즉, 엉뚱한(?) 답변의 가능성이 있는 쪽으로 무게를 늘리는 셈입니다. QA와 같은 작업을 위해 사실기반의 간결한 응답을 원한다면 더 낮은 temperature값을 사용해야겠지요. 반대로 시를 창작하는 등의 창의적인 작업의 경우 temperature 값을 높이는 것이 적합할 것입니다.

**Top_p** - 마찬가지로, 핵 샘플링이라고 불리는 샘플링 기법인 `top_p`에서는 모델이 응답을 생성할 때 결정성을 제어할 수 있습니다. 사실에 근거한 답변을 원한다면 이 값을 낮추고, 더 다채로운 답변을 원한다면 더 높은 값으로 조정하세요.

일반적으로는 둘 중 하나를 변경하는 것을 권장합니다.

기본적인 예시를 살펴보기에 앞서, 사용하는 LLM 버전에 따라 결과가 상이할 수 있음을 알립니다.
